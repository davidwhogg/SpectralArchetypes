\documentclass[12pt]{article}
\usepackage{amssymb,amsmath}

\newcommand{\ampj}{p_j}
\newcommand{\ampij}{q_{ij}}
\newcommand{\chisq}{\chi^2}
\newcommand{\chisqij}{\chisq_{ij}}
\newcommand{\Mvector}[1]{\mathbf{#1}}
\newcommand{\point}{\Mvector{s}}
\newcommand{\spectrumi}{\point_i}
\newcommand{\archetypej}{\Mvector{a}_j}
\newcommand{\xxj}{\Mvector{x}_j}
\newcommand{\Mmatrix}[1]{\mathbb{#1}}
\newcommand{\covari}{\Mmatrix{C}_i}
\newcommand{\covarj}{\Mmatrix{Q}_i}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\invcovari}{\inverse{\covari}}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\minover}[1]{\min_{#1}\,}

\begin{document}

\paragraph{Introductory remarks:}
Typically, in a high-dimensionality data space, a sample of data
consists of points $\spectrumi$, each of which is a measurement of an
object drawn from a distribution of objects, plus noise drawn from a
distribution of noise.  For example, in the case of galaxy spectra,
each spectrum is a measurement of a galaxy, which is drawn from the
distribution of galaxies, plus noise, which is drawn from something
like a gaussian distribution with a different variance for each
dimension (plus some covariances).

As scientific investigators, we almost always want to know about the
underlying distribution of objects, and not the distribution
\emph{convolved} with the noise distribution, which is what we would
get if we treated the sample of measurements as an estimate of the
sample of objects.  In most cases of interest, some of the noise
variances are large enough that we can't ignore the noise when we try
to understand the underlying distribution of objects.  Furthermore,
each data point has a different noise distribution function, in
general, and is therefore drawn from a \emph{different} (convolved)
distribution function.

Furthermore, if the data set is large enough and the dimensionality of
the space is large enough, it becomes exceedingly unlikely that every
data point will have a valid measurement of every dimension.  This
problem of ``missing data'' is not a special case; it is generic to
large data sets.  In the case of galaxy spectra, missing data arise
when pixels have been corrupted by cosmic rays or detector artifacts,
among other things.

Some literature has been written on modeling distribution functions in
the face of missing data (cite refs).  These schemes are
``binary'' in some sense: They treat each dimension of each data point
as either being measured very well, or not measured at all.  In real
data sets, there is a continuum of measurement qualities, and these
qualities can be encoded in each data point's particular noise
distribution function.  In the case of Gaussian noise, the quality of
the measurements is encoded by each data point's inverse covariance
matrix $\invcovari$.

The inverse covariance matrix $\invcovari$ encodes missing data as
eigenvectors (or directions in data space) for which it has zero
eigenvalues, so it is capable of encoding not just missing data values
in particular dimensions, but any linear direction in the
high-dimensionality data space in which the measurement is bad or
invalid.  In the case of accurately Gaussian noise, the inverse
covariance matrix contains \emph{all} of the information about the
measurement quality.  In short, a data analysis scheme---such as some
of those that follow---that properly uses the inverse covariance
matrices \emph{also} properly ignores missing data.

The question I discuss below is ``Can we infer the distribution of
objects, even when each of our measurements is really a sample of that
distribution convolved with that measurement's noise distribution?''.
Or, in other words ``Can we infer a noise-deconvolved distribution
function from a set of noisy measurements?''  The answer is ``yes'' of
course, but it depends on some simplifying assumptions.

\paragraph{The general problem:}
Imagine a set of $N$ measurements $\spectrumi$ (think of them as
galaxy spectra if you are an astronomer), where $1<i<N$, and where
each measurement is a $d$-dimensional column vector (meaning, in this
case, ordered list formatted as a single-column matrix) of $d$ values
(think of them as pixel values on some standard wavelength grid, the
same for every spectrum).  Assume that for each measurement
$\spectrumi$ we also know that the measurement is contaminated by
noise, and we understand well the distribution function for that
noise, which is permitted to be different for every measurement $i$.

At the risk of making some severe philosophical mistake, the
assumption is that for each measurement $\spectrumi$ there is a ``true
value'' $\truthi$ and a displacement due to noise $\noisei$ such that
\begin{equation}
\spectrumi=\truthi+\noisei
\quad ,
\end{equation}
where all three vectors are $d$-dimensional, the true value is drawn
from a distribution $\truedist(\point)$ and the noise displacement is
drawn from a distribution $\noisedisti(\point)$.  The measurement
$\spectrumi$ is therefore drawn from the convolved distribution
$(\truedist * \noisedisti)(\point)$.

%%%%%%%%%

purely gaussian noise, and that the noise variances and covariances
are described accurately by a $M\times M$ inverse covariance matrix
$\invcovari$.  These covariance matrices are different for each
measurement in general.


We want to ``model'' these $N$ spectra as being generated by a
distribution $f(\point)$ in the $M$-dimensional data space that is a
sum of $k$ delta functions located at positions $\archetypej$, or
\begin{equation}
f(\point) = \sum_{j=1}^{k} \ampj\,\delta(\point-\archetypej) \quad ,
\end{equation}
where the $\ampj$ are a set of amplitudes that sum to unity
\begin{equation}
1 = \sum_{j=1}^{k} \ampj \quad .
\end{equation}
(The above is not quite right because we allow an amplitude $\ampij$
to vary in what follows below.)

If the noise is gaussian, the correct measure of ``distance'' between
a spectrum $\spectrumi$ and an archetype $\archetypej$ is $\chisqij$
\begin{equation}
\chisqij = \transpose{(\spectrumi-\archetypej)}
           \cdot\invcovari\cdot(\spectrumi-\archetypej) \quad ,
\end{equation}
where we have assumed that the spectra and the archetypes are all
normalized in some clever way.  Probably better is to renormalize
spectrum-by-spectrum
\begin{equation}
\chisqij = \transpose{(\spectrumi-\ampij\,\archetypej)}
           \cdot\invcovari\cdot(\spectrumi-\ampij\,\archetypej) \quad ,
\end{equation}
where the amplitude $\ampij$ is the best-fit ($\chisqij$-minimizing)
value for each spectrum--archetype pair.  The ``best fit'' archetype
$\archetypej$ for spectrum $\spectrumi$ is the archetype that
minimizes $\chisqij$.  We denote the best-fit archetype index $j=J_i$.

The overall log likelihood of the $N$ spectra $\spectrumi$ given the
$k$ archetypes $\archetypej$ is proportional to the negative of the
total chi-squared sum for all the spectra under each spectrum's
best-fit archetype, or
\begin{equation}
\chisq = \sum_{i=1}^{N} \minover{j}\chisqij
       = \sum_{i=1}^{N}\sum_{j=1}^{k} \delta(j-J_i)\,\chisqij \quad ,
\end{equation}
where the delta-function selects the best-fit archetype.  This
($\chisq$) is the scalar objective we use to compare $k$-archetype
models at fixed $k$ (in the absence of an informative prior).

Now, how do we go about minimizing the scalar objective $\chisq$?
There is a generalization of the $k$-means algorithm that proceeds as
follows.  Imagine that you have a first-guess set of archetypes
$\archetypej$.
\begin{itemize}
\item For each archetype $\archetypej$, take the subset of spectra
$\spectrumi$ for which $\archetypej$ is the best-fit model of
$\spectrumi$, and replace the archetype $\archetypej$ with the
inverse-covariance-weighted mean (see below) of that subset.
\item Re-compute the best-fit amplitudes $\ampij$ and
spectrum--archetype relations $J_i$.
\item Iterate to convergence (that is, no more changes in $J_i$ for any $i$).
\end{itemize}
In case you were wondering, the inverse-covariance-weighed mean is
given by (I think---it is late---I am guessing):
\begin{equation}
\archetypej \leftarrow \covarj\cdot\xxj \quad ,
\end{equation}
where
\begin{equation}
\covarj = \inverse{\left[\sum_{i=0}^N
                         \delta(j-J_i)\,\ampij\,\invcovari\right]} \quad ,
\end{equation}
\begin{equation}
\xxj = \sum_{i=0}^N \delta(j-J_i)\,\invcovari\cdot\spectrumi \quad ,
\end{equation}
and the delta-functions select the set of spectra for which the
best-fit archetype is $\archetypej$.  If replacement by this weighted
mean reduces the chi-squared contribution for the spectra that are
best-fit by $\archetypej$, then I think we can prove that the full
update always leads to convergence to a local minimum.

\end{document}
