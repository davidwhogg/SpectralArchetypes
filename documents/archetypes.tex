\documentclass[12pt]{article}
\usepackage{amssymb,amsmath}

\newcommand{\etal}{\emph{et\,al.}}
\newcommand{\eg}{e.g.}
\newcommand{\ApJ}{\emph{Astrophys.\,J.}}
\newcommand{\Mvector}[1]{\boldsymbol{#1}}
\newcommand{\measurement}{\Mvector{r}}
\newcommand{\truepoint}{\Mvector{\rho}}
\newcommand{\noise}{\Mvector{\Delta}}
\newcommand{\measurementi}{\measurement_i}
\newcommand{\truepointi}{\truepoint_i}
\newcommand{\noisei}{\noise_i}
\newcommand{\mean}{\Mvector{m}}
\newcommand{\meanj}{\mean_j}
\newcommand{\archetypej}{\Mvector{a}_j}
\newcommand{\truedist}{f}
\newcommand{\noisedisti}{g_i}
\newcommand{\gaussian}{\mathcal{N}\!}
\newcommand{\lnlike}{\ln\mathcal{L}}
\newcommand{\lnlikefull}{\ln\mathcal{L}_{\mbox{{\scriptsize FULL}}}}
\newcommand{\ampj}{p_j}
\newcommand{\ampij}{q_{ij}}
\newcommand{\normampij}{p_{ij}}
\newcommand{\chisq}{\chi^2}
\newcommand{\chisqij}{\chisq_{ij}}
\newcommand{\Mmatrix}[1]{\mathbb{#1}}
\newcommand{\varj}{\Mmatrix{V}_j}
\newcommand{\covari}{\Mmatrix{C}_i}
\newcommand{\covarj}{\Mmatrix{Q}_j}
\newcommand{\zero}{\Mmatrix{O}}
\newcommand{\identity}{\Mmatrix{I}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\invvarj}{\inverse{\varj}}
\newcommand{\invcovari}{\inverse{\covari}}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\minover}[1]{\min_{#1}}
\newcommand{\T}{^{\scriptscriptstyle \top}}   % transpose
\newcommand{\trace}{\mathrm{Trace}}

\begin{document}

\paragraph{Introductory remarks:}
Typically, in a high-dimensionality data space, a sample of data
consists of points $\measurementi$, each of which is a measurement of an
object drawn from a distribution of objects, plus noise drawn from a
distribution of noise.  For example, in the case of galaxy spectra,
each spectrum is a measurement of a galaxy, which is drawn from the
distribution of galaxies, plus noise, which is drawn from something
like a gaussian distribution with a different variance for each
dimension (plus some covariances).

As scientific investigators, we almost always want to know about the
underlying distribution of objects, and not the distribution
\emph{convolved} with the noise distribution, which is what we would
get if we treated the sample of measurements as an estimate of the
sample of objects.  In most cases of interest, some of the noise
variances are large enough that we can't ignore the noise when we try
to understand the underlying distribution of objects.  Furthermore,
each data point has a different noise distribution function, in
general, and is therefore drawn from a \emph{different} (convolved)
distribution function.

If the data set is large enough and the dimensionality of the space is
large enough, it becomes exceedingly unlikely that every data point
will have a valid measurement of every dimension.  This problem of
``missing data'' is not a special case; it is generic to large data
sets.  In the case of galaxy spectra, missing data arise when pixels
have been corrupted by cosmic rays or detector artifacts, among other
things.

Some literature has been written on modeling distribution functions in
the face of missing data (cite refs).  These schemes are
``binary'' in some sense: They treat each dimension of each data point
as either being measured very well, or not measured at all.  In real
data sets, there is a continuum of measurement qualities, and these
qualities can be encoded in each data point's particular noise
distribution function.  In the case of Gaussian noise, the quality of
the measurements is encoded by each data point's inverse covariance
matrix $\invcovari$.

The inverse covariance matrix $\invcovari$ encodes missing data as
eigenvectors (or directions in data space) for which it has zero
eigenvalues, so it is capable of encoding not just missing data values
in particular dimensions, but any linear direction in the
high-dimensionality data space in which the measurement is bad or
invalid.  In the case of accurately Gaussian noise, the inverse
covariance matrix contains \emph{all} of the information about the
measurement quality.  In short, a data analysis scheme---such as some
of those that follow---that properly uses the inverse covariance
matrices \emph{also} properly ignores missing data.

The question I discuss below is ``Can we infer the distribution of
objects, even when each of our measurements is really a sample of that
distribution convolved with that measurement's noise distribution?''.
Or, in other words ``Can we infer a noise-deconvolved distribution
function from a set of noisy measurements?''  The answer is ``yes'' of
course, but it depends on some simplifying assumptions.

\paragraph{The general problem:}
Imagine a set of $N$ measurements $\measurementi$ (think of them as
galaxy spectra if you are an astronomer), where $1<i<N$, and where
each measurement is a $d$-dimensional column vector (meaning, in this
case, ordered list formatted as a single-column matrix) of $d$ values
(think of them as pixel values on some standard wavelength grid, the
same for every spectrum).  Assume that for each measurement
$\measurementi$ we also know that the measurement is contaminated by
noise, and we understand well the distribution function for that
noise, which is permitted to be different for every measurement $i$.

At the risk of making some severe philosophical mistake, I am assuming
that for each measurement $\measurementi$ there is a ``true value''
$\truepointi$ and a displacement due to noise $\noisei$ such that
\begin{equation}
\measurementi=\truepointi+\noisei
\quad ,
\end{equation}
where all three vectors are $d$-dimensional, the true value is drawn
from a distribution $\truedist(\truepoint)$, and the noise displacement is
drawn from a distribution $\noisedisti(\noise)$.  The measurement
$\measurementi$ is therefore drawn from the convolved distribution
$(\truedist * \noisedisti)(\measurement)$.

The goal, of course, is to infer the distribution
$\truedist(\truepoint)$ of objects, the distribution of ``true
values'' or the ``error-deconvolved'' distribution.  In what follows I
parameterize the distribution as a mixture of $k$ $d$-dimensional
Gaussians
\begin{equation}
\truedist(\truepoint) =
  \sum_{j=1}^k\ampj\,\gaussian(\truepoint|\meanj,\varj)
\quad \mbox{, where}
\end{equation}
\begin{equation}
1 = \sum_{j=1}^k\ampj
\quad \mbox{, and}
\end{equation}
\begin{equation}
\gaussian(\truepoint|\meanj,\varj) \equiv
  \,\left[(2\pi)^d\,\det(\varj)\right]^{-1/2}
  \,\exp\left(-\frac{1}{2}\,\transpose{[\truepoint-\meanj]}
              \cdot\invvarj\cdot[\truepoint-\meanj]\right)
\quad .
\end{equation}
This parameterization is general, because any continuous distribution
function can be represented as a mixture of Gaussians if $k$ is made
sufficiently large.

As I mentioned in the introduction, I am going to assume that the
noise distribution functions $\noisedisti$ are accurately described by
Gaussians, and that the noise variances and covariances are given by
known $d\times d$ inverse covariance matrices $\invcovari$.  These
covariance matrices are different for each measurement in general.
This is a significant restriction or approximation, but what follows
can be straightforwardly generalized for the case in which the
functions $\noisedisti$ are described not by single Gaussians, but
rather by mixtures of Gaussians, and---as mentioned above---these have
the power to represent any conceivable noise distribution function.

The inference problem now reduces to the optimization problem of
finding the parameters $(\ampj,\meanj,\varj)$ that maximize the (log)
likelihood $\lnlike$ of the noise-convolved distribution functions
$(\truedist * \noisedisti)(\measurement)$ evaluated at the
measurements $\measurementi$.  In the mixture-of-Gaussian framework,
the noise-convolved distribution function for measurement $i$ is
\begin{equation}
(\truedist * \noisedisti)(\measurement) =
  \sum_{j=1}^k\ampj\,\gaussian(\measurement|\meanj,\varj+\covari)
\quad .
\end{equation}
We seek to maximize
\begin{equation}
\lnlike = \sum_{i=1}^N
  \ln\left( \sum_{j=1}^k\ampij \right)
\quad \mbox{, where}
\end{equation}
\begin{equation}\label{eq:ampij}
\ampij\equiv \ampj\,\gaussian(\measurementi|\meanj,\varj+\covari)
\quad .
\end{equation}

This choice of scalar for optimization is made independently of any
choice of \emph{methodology} for optimization, but for all cases of
interest, there is some expectation-maximization-like algorithm
possible.

\paragraph{Standard algorithms as limits of the general problem:}
Some of the standard data analysis or machine-learning algorithms in
wide use at the present day are solutions to \emph{limits} of the
problem stated generally above.  Here are a few:

Take the limit in which the measurements have negligible noise;
that is, set the measurement covariance matrices $\covari$ to zero
\begin{equation}
\covari = \zero \quad \mbox{, where}
\end{equation}
\begin{equation}
\zero \equiv \lim_{\epsilon \rightarrow 0} [\epsilon\,\identity]
\quad ,
\end{equation}
and $\identity$ is the $d$-dimensional identity matrix.  This is
equivalent to working in the limit in which the variance of the
distribution $\truedist(\truepoint)$ is much larger in every direction
than the variance of any measurement's noise distribution
$\noisedisti(\noise)$.  In the limit $\covari=\zero$ and $k=1$, the
problem is solved exactly by \emph{principal components analysis}.

In the limit $\covari=\zero$ and $\varj=\zero$, the problem is solved
exactly by \emph{K-means}.  K-means involves hard assignment.  When
the variances become very small, the factors $\ampij$ become
exceedingly small but also exceedingly differentiated, such that all
of the weight $\sum_j\ampij$ comes, for each measurement $i$, from the
closest of the $k$ gaussian means $\meanj$.

In the limit $\covari=\zero$, the problem is solved by the usual,
straightforward methodology known as \emph{mixture-of-Gaussians
expectation-maximization}, the workhorse of machine learning.

In the limit $\varj=\zero$ and $k=1$, the problem is solved by
\emph{taking the inverse-covariance-weighted mean} of the
measurements, or
\begin{equation}
\mean_1 = \inverse{\left[
  \sum_{i=0}^N \invcovari
\right]}\cdot\left[
  \sum_{i=0}^N \invcovari\cdot\measurementi
\right]
\quad .
\end{equation}

\paragraph{The archetypes problem:}
The fully general problem is being worked on by Bovy, and perhaps has
a complete solution already (Hogg \etal, 2005, \ApJ, 629, 268).  I am
currently interested in the limit $\varj=\zero$, where the
distribution of ``true values'' is being approximated as a set of
delta-functions.  In this case, I \emph{conjecture} that the problem
is solved by iterating this procedure:
\begin{equation}
\ampij \leftarrow \ampj\,\gaussian(\measurementi|\meanj,\covari)
\end{equation}
\begin{equation}
\normampij \leftarrow \inverse{\left[
  \sum_{j=1}^k\ampij
\right]}\,\ampij
\end{equation}
\begin{equation}
\ampj \leftarrow \frac{1}{N}\,\sum_{i=1}^N\normampij
\end{equation}
\begin{equation}
\meanj \leftarrow \inverse{\left[
  \sum_{i=1}^N\normampij\,\invcovari
\right]}\,\left[
  \sum_{i=1}^N\normampij\,\invcovari\cdot\measurementi
\right]
\end{equation}


\paragraph{Proof of the algorithm above:}
That the updates steps above lead to a solution of the problem in the case
that $\varj=\zero$ can be proved as follows. Introduce the indicator
variables $\normampij$ such that
\begin{equation}
\normampij = \left\{
\begin{array}{rl}
1 & \text{if data point $i$ was generated by delta function $j$}\\
0 & \text{if data point $i$ was \emph{not} generated by delta function $j$}\\
\end{array} \right.
\end{equation}
This variable can take on values between these extreme values, in
which case $\normampij$ corresponds to the probability that data point
$i$ was generated by the delta function $j$. In any case, for every
data point we have that $\sum_j \normampij = 1$. Using these indicator
variables as ``hidden'' variables we can now write the ``full data''
likelihood as
\begin{equation}
\lnlikefull = \sum_{i=1}^N \sum_{j=1}^K \normampij\,\ln \ampij\, ,
\end{equation}
in which $\ampij$ is still given by (\ref{eq:ampij}). Using Jensen's
inequality (\eg, MacKay 2003) we can show that setting
\begin{equation}
\normampij \leftarrow \inverse{\left[
  \sum_{j=1}^k\ampij
\right]}\,\ampij
\end{equation}
ensures that optimizing the full data likelihood with respect to the
model parameters is equivalent to optimizing the likelihood (see Bovy
\etal, 2009). The $\ampij$ are optimized by setting
\begin{equation}
\ampj \leftarrow \frac{1}{N}\,\sum_{i=1}^N\normampij
\end{equation}
(\eg, Bovy \etal, 2009). Optimization of the mean reduces to optimizing
\begin{equation}
\sum_i \sum_j \normampij \left[\ln \det \covari + (\measurementi-\meanj)\T\invcovari (\measurementi - \meanj)\right] \ ,
\end{equation}
thus, solving
\begin{equation}
0 = \sum_i \normampij \trace\left[ - 2 \invcovari (\measurementi - \meanj) d \meanj\T \right] \ ,
\end{equation}
such that the update step becomes
\begin{equation}
\meanj \leftarrow \inverse{\left[
  \sum_{i=1}^N\normampij\,\invcovari
\right]}\,\left[
  \sum_{i=1}^N\normampij\,\invcovari\cdot\measurementi
\right]\, .
\end{equation}


\end{document}
