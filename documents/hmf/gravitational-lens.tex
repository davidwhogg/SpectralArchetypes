\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,mathrsfs}
\newcommand{\facility}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
 \newcommand{\apriori}{\foreign{a priori}}
\newcommand{\equationname}[1]{equation~(\ref{#1})}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\hoggvector}[1]{\boldsymbol{\vec{#1}}}
 \newcommand{\avec}{\hoggvector{a}}
 \newcommand{\Fvec}{\hoggvector{F}}
 \newcommand{\fvec}{\hoggvector{f}}
 \newcommand{\gvec}{\hoggvector{g}}
\newcommand{\hoggmatrix}[1]{\boldsymbol{#1}}
 \newcommand{\Amatrix}{\hoggmatrix{A}}
 \newcommand{\Cmatrix}{\hoggmatrix{C}}
 \newcommand{\Gmatrix}{\hoggmatrix{G}}
 \newcommand{\Qmatrix}{\hoggmatrix{Q}}
 \newcommand{\Sigmamatrix}{\hoggmatrix{\Sigma}}
\newcommand{\like}{\mathscr{L}}
\begin{document}

\section{Introduction}

TBD: What is the problem and why is it important?

The standard data-driven models for quasars---or any other kind of
spectroscopic object in astronomy---is the highest-ranked principal
components from a principal components analysis (PCA) or equivalent.
The PCA has one advantage and a number of drawbacks.  The advantage is
that it is entirely data-driven: The construction of the PCA requires
no theoretical or external knowledge about the spectra being modeled;
it is a dimensionality reduction in the space of the observed spectra.

There are many drawbacks to PCA but the most important is that the PCA
returns the principal directions---the eigenvectors with maximum
eigenvalues---of the variance tensor of the data; this variance tensor
has contributions from intrinsic variation among spectra, and
contributions from observational noise.  That is, a direction in
spectrum space can enter into the top principal components because it
is a direction of great astrophysical variation, or because there is a
lot of noise in the observations along that direction, or both.  PCA
is agnostic about the \emph{source} of the variance, while astronomers
are not; astronomers want to know about the astrophysical processes
that generate the data \emph{prior} to the addition of observational
noise.

Other drawbacks to PCA include the following: It treats the data as
drawn from a linear subspace of the full spectral space.  This
assumption is unlikely to be true in any application.  It also has
trouble separating the spectral variation that comes from amplitude
changes (overall flux or luminosity changes) as distinct from
variations that come from shape changes in the spectra.  Various hacks
have been employed to deal with this, but many of them make the linear
subspace assumption even less valid than it was \foreign{a priori}.
Finally, PCA has no idea about prior information; it is just as happy
creating components with negative amplitudes as positive amplitudes
and the linear subspace therefore contains many quadrants, in general,
that represent spectra with completely unphysical properties (such as
negative emission lines and the like).

In this Note, we use a data-driven model for quasar spectra
that overcomes most---though not all---of the problems with PCA.

\section{Data}

Each of the $N$ spectra $i$ can be thought of as an ordered list or
column vector $\fvec_i$ of $M$ flux density (energy per area per time
per wavelength) measurements $f_{ij}$ on a grid of $M$ observer-frame
wavelengths $\lambda^{\mathrm{obs}}_j$:
\begin{equation}
\fvec_i
\equiv \left[\begin{array}{c} f_{i1} \\
                              f_{i2} \\
                              \cdots \\
                              f_{iM} \end{array}\right]
\equiv \left[\begin{array}{c} f_{\lambda,i}(\lambda^{\mathrm{obs}}_1) \\
                              f_{\lambda,i}(\lambda^{\mathrm{obs}}_2) \\
                                                \cdots \\
                              f_{\lambda,i}(\lambda^{\mathrm{obs}}_M) \end{array}\right]
\quad ,
\end{equation}
where the wavelength grid can be arbitrary but has been made uniform in
the logarithm $\ln\lambda^{\mathrm{obs}}$ for simplicity of making
redshift changes.  Associated with each measurement $f_{ij}$ is an
uncertainty variance $\sigma_{ij}$ and we will assume in what follows
that these uncertainty variances are well measured and that the
uncertainties are essentially Gaussian.  We will assume that
off-diagonal terms (covariances) in the uncertainty variance tensor
are small, or that the uncertainty variance tensor (covariance matrix)
$\Cmatrix_i$ is approximately
\begin{equation}
\Cmatrix_i =
 \left[\begin{array}{cccc} \sigma_{i1}^2 & 0 & & 0 \\
                           0 & \sigma_{i2}^2 & & 0 \\
                           & & \cdots & \\
                           0 & 0 & & \sigma_{iM}^2 \end{array}\right]
\quad .
\end{equation}

\section{Spectral model}

Each spectrum is modeled as the linear superposition of one or more
objects at different redshifts.  We want to model the spectrum of each
object (galaxy or quasar) with a sum of $K$ linear components:
\begin{equation}
f_{\lambda}(\lambda) = \sum_{k=1}^{K} a_k\,g_k(\lambda)
\quad ,
\end{equation}
where the modeling is done implicitly in the object rest frame, the
$a_k$ are coefficients, and the $g_k(\lambda)$ are basis spectra.
Given basis spectra, the best set of coefficients for any observed
spectrum---under the assumption of known, Gaussian uncertainties---are
found by least-square fitting.  The challenge is to find the
best set of basis spectra.

Often in astronomy, this basis is found by principal components
analysis (or equivalent) and then selection of the largest-variance
components or largest-eigenvalue eigenvectors.  However, this use of
PCA naturally locates the $K$-dimensional linear basis that minimizes
the mean-squared error in the space in which all pixels of all spectra
are treated equally: They are weighted equally in the analysis, and
residuals in them are minimized by the PCA with equal aggression.
This is an inappropriate approach in the real situation in which
different data points come with very different uncertainty variances,
and it is absolutely inapplicable when there are missing data---as
there \emph{always} are in real data sets.

For these reasons, we seek to find the basis set that optimizes a
justified scalar objective, one that is consistent with the individual
spectral pixel uncertainty variances and with the fact that there are
missing data.  When uncertainties are gaussian with known variances,
the logarithm of the likelihood is proportional to chi-squared, so we
seek to find the basis functions and coefficients that minimize a
total chi-squared:
\begin{eqnarray}\displaystyle
\chi^2 & = & X - 2\,\ln\like \nonumber\\
 & = & \sum_{i=1}^N \sum_{m=1}^M
\frac{\left[f_{im}-\sum_{k=1}^K a_{ik}
                      \,g_k(\lambda_j/[1+z_i])\right]^2}
{\sigma^2_{ij}}
\quad ,
\end{eqnarray}
where $X$ is some constant and we have implicitly assumed that each
spectrum $i$ under consideration at this stage is well explained by
having all its flux come from a single object at a known redshift
$z_i$.  The free parameters are the $N\,K$ coefficients $a_{ik}$ and
the $K$ functions $g_k(\lambda)$.  Roughly speaking, we seek to find
the coefficients and basis functions that globally minimize this
scalar $\chi^2$.

Precisely speaking, we make two adjustments to this goal.  The first
is that we can't demand global optimization; this problem is not
convex.  Indeed, there are enormous numbers of local minima, in both
the trivial sense that there are exact degeneracies (swap two basis
functions and their corresponding coefficients, or re-scale a basis
function and the corresponding coefficients, and so on) and in the
non-trivial sense that there are qualitatively different solutions.
All that our methods (described in detail below) guarantee is that we
have, \emph{at fixed coefficients $a_{ik}$} the globally optimal basis
functions $g_k(\lambda)$ and that we have, at fixed basis functions
$g_k(\lambda)$ the globally optimal coefficients $a_{ik}$.

The second adjustment is that we impose a smoothness prior to improve
performance at (rest-frame) wavelengths at which we have very few
data.  In practice, we implement this prior by constructing the basis
functions $g_k(\lambda)$ on a grid of $M_g>M$ rest wavelengths
$\lambda_{\ell}$ and penalizing quadratically large pixel-to-pixel
variations.  That is, we optimize not the pure $\chi^2$ above but a
modified scalar $\chi_{\epsilon}^2$
\begin{equation}
\chi_{\epsilon}^2 \equiv \chi^2
 + \epsilon\,\sum_{k=1}^K \sum_{\ell=2}^{M_g}
 \left[g_k(\lambda_{\ell})-g_k(\lambda_{\ell-1})\right]^2
\quad ,
\end{equation}
where $\epsilon$ is a scalar that sets the strength of the smoothing.
Optimization of this scalar $\chi_{\epsilon}^2$ is equivalent to
optimization of the posterior probability distribution with a Gaussian
prior applied to the pixel-to-pixel differences.  As with the observed
spectra, we pixelize the basis functions $g_k(\lambda)$ on a grid that
is uniform in logarithmic wavelength $\ln\lambda$ (with the same grid
spacing as that used for the observed spectra).

discussion of Gaussian Processes and this being like a prior

possibility of restricting to non-negative components and coefficients.

\section{Optimization}

options for optimizing: block-diagonal (or nearly so) least-square
fitting a-la astep(); iterated gradient descent a-la astepgd().

options for optimizing non-negative case.

options for initializing: Kmeans, PCA, random spectra, basis functions.  Issues with each.

degeneracies in subspace description when not non-negative:  We break these how?  Normalization, rotation, etc.

\section{Application}

To assess the power of the technique, we are going to confirm known double-redshift objects in the SDSS spectroscopic sample. More specifically, by using the method presented above, we define a small number of components that is sufficient for modeling the SDSS spectra. Using these components we fit each observed spectrum at the redshift provided by SDSS. Then we repeat the fitting, but this time using one set of components at the SDSS redshift and one set of components at values of redshift that lie on a nominal grid. If a second object is present we expect the fit to be improved when we use two sets of components at the redshifts of SDSS and that of the fainter object. In the example that follows we will demonstrate this using the SLACS sample.

\subsection{Training}

In order to detect the presence of two objects at different redshifts in the SDSS spectra, we need to be able to model the spectra of all types of objects that have been observed by the survey (galaxies, LRGs and QSOs). To do so, we have to train our method separately for each class. For this purpose we selected a small random sample of spectra for each type of object (5000 for galaxies and LRGs and 11000 for QSOs, the numbers were selected as such in order to make sure that we have at least as many objects as number of pixels, which was needed for some of the tests we performed using PCA). The values of redshift were selected to be in the ranges of  0.01-0.06, 0.20-0.50 and 0.50-5.00 for galaxies, LRGs and QSOs respectively. The sample was used to determine the maximum wavelength coverage, i.e. a wavelength area for which at least 10 sources have valid data at the beginning and the end of the spectrum. In this way we are able to fit the part of the spectrum that is produced by the second object for a large range of redshift values. This has as a result the presence of missing data in almost all the spectra of our sample. To deal with this problem we have set the fluxes at those areas equal to the first or the last non-masked pixel of the spectrum and we have set the noise of those pixels to a very high value ($10^{-12} erg/sec/cm^2/\AA$), so that they will not be taken into account by the method. The final wavelength coverage for each object is: 3580.964-9109.615 \AA\ for galaxies, 2544.486-7615.528 \AA\ for LRGs and 1216.186-6109.420 \AA\ for QSOs, corresponding to 4056, 4762 and 7011 pixels respectively.

Using a number of spectra equal to the number of pixels selected for each source, we performed PCA on those data. The PCA results were used as an initialization to our method. The method was run for a subset of approximately 1000 spectra of each source for a different number of components and for 16 iterations, which seems to be enough for the method to converge. This can be seen in the \textbf{figures} below in which we present the results of the fitting (total $\chi^2$, i.e. the sum of $\chi^2$ values over all wavelengths and all spectra) of the 1000 spectra of our sample with a different number of components. This test was also performed for four different values (1,3,10 and 30) of the $\epsilon$ smoothing factor.

In these \textbf{figures} we can see that the fitting of the spectra improves a lot even after the first iteration, indicating that using this method and a given number of components we can achieve a better modeling of the spectra than with the PCA. In the \textbf{figures} that follow we present our new set of components plotted over the initial PCA components. We should point out that a straight comparison between the components extracted by the two methods is not meaningful since they correspond to different subspaces of the observed data and that the comparison between the methods can be achieved only by using the results of the fitting to a set of spectra.

In \textbf{figures} we also present how the components and the total $\chi^2$ value change with the value of the smoothing factor $\epsilon$. As was expected, by increasing the value of the $\epsilon$ parameter, and therefore the smoothness of the resulting components, we put more constraints on them, leading to worse fits of the data.

In the results presented above (\textbf{figures}) we have used the same set of data to train as well as to test the method. As a first step towards cross-validation we used a new random set of 1000 spectra as a testing set. The results of the fitting of this set, at each iteration, with the components we extracted with the method and the same training set of spectra as before, are presented in \textbf{figures}. In these plots with different colors we present the results for different values of $\epsilon$. As we can see in the new test set the best fit is achieved by different values of $\epsilon$ and not for the smallest one as before.

In order to check how much the method depends on the initialization we repeated our tests using different sets of components as our initial basis. In \textbf{figures} we present the results of the fitting for the new test set when we used a random set of spectra, the output of the K-means algorithm and a set of sin and cosin functions to initialize the method. More specifically in the case that a set of random spectra was used, we selected the ones that include information at the reddest or the bluest part of the spectrum. In the case of the K-means initialization we used the algorithm kmeans(stats) implemented in R with a number of centers equal to the number of components needed. The algorithm uses a random set of points as its initialization and therefore the results are different in every run.

By comparing these results we see that they are becoming worse as we change the initialization from the PCA output, to the random spectra, to the K-means results and to the sin and cosin functions. This result is expected since the Principal Components results are chosen in a way to increase the percentage of the total variance they include. The most probable reason why the random spectra seem to be a better initialization than the K-means output is that we have chosen spectra that include information at the ends of the wavelength coverage, something that is probably not true in the K-means initialization where the centers are defined mainly by spectra with constant values at these areas. The sin cosin functions lead to the worst results as expected since they include the least information compared to all the initializations used here.

As a last test of the method we checked how a non-negative constraint affects the results. Since negative values in the spectra are caused by the observations, modeling the spectra of astronomical sources with components that include negative values has no physical meaning. This problem can be solved be applying a non-negative constraint to our basis. The way that this is achieved is by initializing with a non-negative set of components and coefficients and iterating according to \textbf{equation}. One of the best ways to initialize this method with a set of non-negative components that include physical information is to use once again the K-means algorithm. The results of the fitting of the test set of spectra with the components extracted in this way are presented in \textbf{figures}, while in \textbf{figures} we present the resulting components for each type of objects for K=7.

By comparing those results with the ones obtained without the non-negative constraint we see that the fitting is now worse. This was of course expected since this is a very strict constrain. On the other hand even if the components now seem to have a better physical meaning, i.e. they look more like spectra of particular types of objects, in many cases there seems to be a problem at the edges of the spectra where they tend to start from exactly zero values. At this point we should mention that when applying the method for the non-negative case we have not used an additional smoothing constraint.

Based on the results of all the tests presented here we have decided to fit the SDSS spectra using the 7, 6 and 8 (?) components that were produced by our method for galaxy, LRG and QSO sources after 16 iterations and using $\epsilon$=1 \textbf{(the numbers will change)}. A more detailed description of the fitting and its results is presented below.

\subsection{Hypothesis test}

For each spectrum $i$, we generate $Z+1$ mutually exclusive hypotheses:
The null hypothesis $S_i$ that spectrum $i$ only has significant flux
coming from a single redshift $z_i$ determined by the \facility{SDSS}
pipeline, and $Z$ hypotheses $D_{ij}$ that spectrum $i$ has significant
flux coming from two redshifts, $z_i$ and another redshift
$z_j>z_i+\epsilon$.  In the context of this very restricted universe
of hypotheses, the odds ratio $\Omega_i$ for the null hypothesis is
\begin{equation}\label{eq:odds}
\Omega_i = \frac{\sum_{j=1}^Z p(D_{ij}|\fvec_i,I)}{p(S_i|\fvec_i,I)}
 = \sum_{j=1}^Z \left[\frac{p(\fvec_i|D_{ij},I)}{p(\fvec_i|S_i,I)}
 \,\frac{p(D_{ij}|I)}{p(S_i|I)}\right] = \sum_{j=1}^Z\Omega_{ij}\quad,
\end{equation}
where the spectral flux data are represented by the vector
$\fvec_i$, the symbol $I$ represents all of the prior information
in the problem, including but not limited to the hypothesis
specification, the wavelengths and uncertainties associated with the
spectral flux data, and any other knowledge that the investigator
might have about the hypotheses prior to any data analysis.  We have
implicitly defined an individual-hypothesis odds ratio
\begin{equation}
\Omega_{ij} = \frac{p(\fvec_i|D_{ij},I)}{p(\fvec_i|S_i,I)}
  \,\frac{p(D_{ij}|I)}{p(S_i|I)}\quad.
\end{equation}
Because an individual spectrum is unlikely \apriori\ to show two
redshifts, the prior probabilities will have the asymmetry
\begin{equation}
\sum_{j=1}^Z p(D_{ij}|I) \ll p(S_i|I) \quad,
\end{equation}
and it remains for us to decide how to set the relative prior
probabilities among the $Z$ hypotheses $D_{ij}$.

We have split the sum in the odds $\Omega_i$ into a sum of individual
odds ratios $\Omega_{ij}$ because, as we will see, we need to estimate
the ratio for each $j$ individually; we can't just evaluate the total
numerator and denominator of $\Omega_i$ independently.  The reason for
this is the all-important \emph{spectral coverage}.  Each setting of
the pair $(z_i,z_j)$ limits differently the spectral range over which
the eigenspectra are both well determined.  Imagine that one of the
hypotheses $D_{ij}$ is only testable on some particular spectral range
$[\lambda_{\min},\lambda_{\max}]$.  The data in this spectral range
can be used to estimate the single-hypothesis odds ratio $\Omega_{ij}$
of hypothesis $D_{ij}$ to the null hypothesis $S_i$.  Clearly
hypotheses $D_{ij}$ that are testable with larger spectral ranges will
be better tested, but the fact that different hypotheses are subject
to tests of different strengths merely weakens---does not
invalidate---the total hypothesis test.

The SDSS spectra have near-Gaussian uncertainties.
Therefore, for each hypothesis $D_{ij}$ we can perform least-square
fitting ($\chi^2$ minimization) on the subset of $N_{ij}$ pixels in
the flux vector $\fvec_i$ that overlap the eigenspectra spectral
ranges for both redshifts $z_i$ and $z_j$.  If we perform the
least-square fit with $n$ eigenspectra at each redshift, then the odds
ratio can be approximated by a modified difference in $\chi^2$:
\begin{equation}
\ln\Omega_{ij}= \frac{1}{2}\,\left[\chi^2_i-\chi^2_{ij}-n\right]
 +\ln\frac{p(D_{ij}|I)}{p(S_i|I)} \quad,
\end{equation}
where we have taken the natural logarithm to simplify things,
$\chi^2_i$ is the minimum $\chi^2$ under hypothesis $S_i$,
$\chi^2_{ij}$ is the minimum $\chi^2$ under hypothesis $D_{ij}$, the
adjustment of $-n$ accounts for the fact that the $S_i$ fit has $n$
fewer parameters than the $D_{ij}$ fit, and the last term is the prior
ratio.  Importantly, in this odds-ratio expression, the $\chi^2$ fits
for hypotheses $S_i$ and $D_{ij}$ must have been performed \emph{over
the same $N_{ij}$ pixels in both cases}.  Even then, the expression is
something of an approximation, because it effectively assumes that the
data are affected by perfectly known, perfectly Gaussian noise and
that one of the two hypotheses is capable of providing a good fit to
the data.

\section{Discussion points}

Objectively choosing K

Train all data, subsample (practical reasons)

Compare to PCA; compare by comparing the quality of fits, not the basis functions themselves.

Search for the secondery object also at lower redshift than the primary

Other application SMBHB, redshift determination

Priors on amplitudes

Prior for non negativity (spectra, amplitudes)

Convexity

Slower than PCA
\end{document}
