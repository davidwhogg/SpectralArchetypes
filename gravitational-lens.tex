\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,mathrsfs}
\newcommand{\facility}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
 \newcommand{\apriori}{\foreign{a priori}}
\newcommand{\equationname}[1]{equation~(\ref{#1})}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\hoggvector}[1]{\boldsymbol{\vec{#1}}}
 \newcommand{\avec}{\hoggvector{a}}
 \newcommand{\Fvec}{\hoggvector{F}}
 \newcommand{\fvec}{\hoggvector{f}}
 \newcommand{\gvec}{\hoggvector{g}}
\newcommand{\hoggmatrix}[1]{\boldsymbol{#1}}
 \newcommand{\Amatrix}{\hoggmatrix{A}}
 \newcommand{\Cmatrix}{\hoggmatrix{C}}
 \newcommand{\Gmatrix}{\hoggmatrix{G}}
 \newcommand{\Qmatrix}{\hoggmatrix{Q}}
 \newcommand{\Sigmamatrix}{\hoggmatrix{\Sigma}}
\newcommand{\like}{\mathscr{L}}
\begin{document}

\section{Introduction}

TBD: What is the problem and why is it important?

The standard data-driven models for quasars---or any other kind of
spectroscopic object in astronomy---is the highest-ranked principal
components from a principal components analysis (PCA) or equivalent.
The PCA has one advantage and a number of drawbacks.  The advantage is
that it is entirely data-driven: The construction of the PCA requires
no theoretical or external knowledge about the spectra being modeled;
it is a dimensionality reduction in the space of the observed spectra.

There are many drawbacks to PCA but the most important is that the PCA
returns the principal directions---the eigenvectors with maximum
eigenvalues---of the variance tensor of the data; this variance tensor
has contributions from intrinsic variation among spectra, and
contributions from observational noise.  That is, a direction in
spectrum space can enter into the top principal components because it
is a direction of great astrophysical variation, or because there is a
lot of noise in the observations along that direction, or both.  PCA
is agnostic about the \emph{source} of the variance, while astronomers
are not; astronomers want to know about the astrophysical processes
that generate the data \emph{prior} to the addition of observational
noise.

Other drawbacks to PCA include the following: It treats the data as
drawn from a linear subspace of the full spectral space.  This
assumption is unlikely to be true in any application.  It also has
trouble separating the spectral variation that comes from amplitude
changes (overall flux or luminosity changes) as distinct from
variations that come from shape changes in the spectra.  Various hacks
have been employed to deal with this, but many of them make the linear
subspace assumption even less valid than it was \foreign{a priori}.
Finally, PCA has no idea about prior information; it is just as happy
creating components with negative amplitudes as positive amplitudes
and the linear subspace therefore contains many quadrants, in general,
that represent spectra with completely unphysical properties (such as
negative emission lines and the like).

In this Note, we use a data-driven model for quasar spectra
that overcomes most---though not all---of the problems with PCA.

\section{Data}

Each of the $N$ spectra $i$ can be thought of as an ordered list or
column vector $\fvec_i$ of $M$ flux density (energy per area per time
per wavelength) measurements $f_{ij}$ on a grid of $M$ observer-frame
wavelengths $\lambda^{\mathrm{obs}}_j$:
\begin{equation}
\fvec_i
\equiv \left[\begin{array}{c} f_{i1} \\
                              f_{i2} \\
                              \cdots \\
                              f_{iM} \end{array}\right]
\equiv \left[\begin{array}{c} f_{\lambda,i}(\lambda^{\mathrm{obs}}_1) \\
                              f_{\lambda,i}(\lambda^{\mathrm{obs}}_2) \\
                                                \cdots \\
                              f_{\lambda,i}(\lambda^{\mathrm{obs}}_M) \end{array}\right]
\quad ,
\end{equation}
where the wavelength grid can be arbitrary but has been made uniform in
the logarithm $\ln\lambda^{\mathrm{obs}}$ for simplicity of making
redshift changes.  Associated with each measurement $f_{ij}$ is an
uncertainty variance $\sigma_{ij}$ and we will assume in what follows
that these uncertainty variances are well measured and that the
uncertainties are essentially Gaussian.  We will assume that
off-diagonal terms (covariances) in the uncertainty variance tensor
are small, or that the uncertainty variance tensor (covariance matrix)
$\Cmatrix_i$ is approximately
\begin{equation}
\Cmatrix_i =
 \left[\begin{array}{cccc} \sigma_{i1}^2 & 0 & & 0 \\
                           0 & \sigma_{i2}^2 & & 0 \\
                           & & \cdots & \\
                           0 & 0 & & \sigma_{iM}^2 \end{array}\right]
\quad .
\end{equation}

\section{Spectral model}

Each spectrum is modeled as the linear superposition of one or more
objects at different redshifts.  We want to model the spectrum of each
object (galaxy or quasar) with a sum of $K$ linear components:
\begin{equation}
f_{\lambda}(\lambda) = \sum_{k=1}^{K} a_k\,g_k(\lambda)
\quad ,
\end{equation}
where the modeling is done implicitly in the object rest frame, the
$a_k$ are coefficients, and the $g_k(\lambda)$ are basis spectra.
Given basis spectra, the best set of coefficients for any observed
spectrum---under the assumption of known, Gaussian uncertainties---are
found by least-square fitting.  The challenge is to find the
best set of basis spectra.

Often in astronomy, this basis is found by principal components
analysis (or equivalent) and then selection of the largest-variance
components or largest-eigenvalue eigenvectors.  However, this use of
PCA naturally locates the $K$-dimensional linear basis that minimizes
the mean-squared error in the space in which all pixels of all spectra
are treated equally: They are weighted equally in the analysis, and
residuals in them are minimized by the PCA with equal aggression.
This is an inappropriate approach in the real situation in which
different data points come with very different uncertainty variances,
and it is absolutely inapplicable when there are missing data---as
there \emph{always} are in real data sets.

For these reasons, we seek to find the basis set that optimizes a
justified scalar objective, one that is consistent with the individual
spectral pixel uncertainty variances and with the fact that there are
missing data.  When uncertainties are gaussian with known variances,
the logarithm of the likelihood is proportional to chi-squared, so we
seek to find the basis functions and coefficients that minimize a
total chi-squared:
\begin{eqnarray}\displaystyle
\chi^2 & = & X - 2\,\ln\like \nonumber\\
 & = & \sum_{i=1}^N \sum_{m=1}^M
\frac{\left[f_{im}-\sum_{k=1}^K a_{ik}
                      \,g_k(\lambda_j/[1+z_i])\right]^2}
{\sigma^2_{ij}}
\quad ,
\end{eqnarray}
where $X$ is some constant and we have implicitly assumed that each
spectrum $i$ under consideration at this stage is well explained by
having all its flux come from a single object at a known redshift
$z_i$.  The free parameters are the $N\,K$ coefficients $a_{ik}$ and
the $K$ functions $g_k(\lambda)$.  Roughly speaking, we seek to find
the coefficients and basis functions that globally minimize this
scalar $\chi^2$.

Precisely speaking, we make two adjustments to this goal.  The first
is that we can't demand global optimization; this problem is not
convex.  Indeed, there are enormous numbers of local minima, in both
the trivial sense that there are exact degeneracies (swap two basis
functions and their corresponding coefficients, or re-scale a basis
function and the corresponding coefficients, and so on) and in the
non-trivial sense that there are qualitatively different solutions.
All that our methods (described in detail below) guarantee is that we
have, \emph{at fixed coefficients $a_{ik}$} the globally optimal basis
functions $g_k(\lambda)$ and that we have, at fixed basis functions
$g_k(\lambda)$ the globally optimal coefficients $a_{ik}$.

The second adjustment is that we impose a smoothness prior to improve
performance at (rest-frame) wavelengths at which we have very few
data.  In practice, we implement this prior by constructing the basis
functions $g_k(\lambda)$ on a grid of $M_g>M$ rest wavelengths
$\lambda_{\ell}$ and penalizing quadratically large pixel-to-pixel
variations.  That is, we optimize not the pure $\chi^2$ above but a
modified scalar $\chi_{\epsilon}^2$
\begin{equation}
\chi_{\epsilon}^2 \equiv \chi^2
 + \epsilon\,\sum_{k=1}^K \sum_{\ell=2}^{M_g}
 \left[g_k(\lambda_{\ell})-g_k(\lambda_{\ell-1})\right]^2
\quad ,
\end{equation}
where $\epsilon$ is a scalar that sets the strength of the smoothing.
Optimization of this scalar $\chi_{\epsilon}^2$ is equivalent to
optimization of the posterior probability distribution with a Gaussian
prior applied to the pixel-to-pixel differences.  As with the observed
spectra, we pixelize the basis functions $g_k(\lambda)$ on a grid that
is uniform in logarithmic wavelength $\ln\lambda$ (with the same grid
spacing as that used for the observed spectra).

discussion of Gaussian Processes and this being like a prior

possibility of restricting to non-negative components and coefficients.

\section{Optimization}

options for optimizing: block-diagonal (or nearly so) least-square
fitting a-la astep(); iterated gradient descent a-la astepgd().

options for optimizing non-negative case.

options for initializing: Kmeans, PCA, random spectra, basis functions.  Issues with each.

degeneracies in subspace description when not non-negative:  We break these how?  Normalization, rotation, etc.

\section{Application}

To demonstrate the power of the technique, we are going to confirm known double-redshift objects in the SDSS spectroscopic sample.

\subsection{Training}

Vivi: How are the training data selected?

Vivi: How do we initialize, how do we optimize, how do we know we have converged?

Vivi: show results as a function of epsilon, K, iteration number.  Need to explain that you shouldn't compare basis functions; you should compare fits to real objects.

Vivi: show non-negative, other initializations, etc

Vivi: baby step towards cross-validation:  Define a disjoint test set.  How do our fits to *new* data vary with epsilon, K, nonnegative?

\subsection{Hypothesis test}

For each spectrum $i$, we generate $Z+1$ mutually exclusive hypotheses:
The null hypothesis $S_i$ that spectrum $i$ only has significant flux
coming from a single redshift $z_i$ determined by the \facility{SDSS}
pipeline, and $Z$ hypotheses $D_{ij}$ that spectrum $i$ has significant
flux coming from two redshifts, $z_i$ and another redshift
$z_j>z_i+\epsilon$.  In the context of this very restricted universe
of hypotheses, the odds ratio $\Omega_i$ for the null hypothesis is
\begin{equation}\label{eq:odds}
\Omega_i = \frac{\sum_{j=1}^Z p(D_{ij}|\fvec_i,I)}{p(S_i|\fvec_i,I)}
 = \sum_{j=1}^Z \left[\frac{p(\fvec_i|D_{ij},I)}{p(\fvec_i|S_i,I)}
 \,\frac{p(D_{ij}|I)}{p(S_i|I)}\right] = \sum_{j=1}^Z\Omega_{ij}\quad,
\end{equation}
where the spectral flux data are represented by the vector
$\fvec_i$, the symbol $I$ represents all of the prior information
in the problem, including but not limited to the hypothesis
specification, the wavelengths and uncertainties associated with the
spectral flux data, and any other knowledge that the investigator
might have about the hypotheses prior to any data analysis.  We have
implicitly defined an individual-hypothesis odds ratio
\begin{equation}
\Omega_{ij} = \frac{p(\fvec_i|D_{ij},I)}{p(\fvec_i|S_i,I)}
  \,\frac{p(D_{ij}|I)}{p(S_i|I)}\quad.
\end{equation}
Because an individual spectrum is unlikely \apriori\ to show two
redshifts, the prior probabilities will have the asymmetry
\begin{equation}
\sum_{j=1}^Z p(D_{ij}|I) \ll p(S_i|I) \quad,
\end{equation}
and it remains for us to decide how to set the relative prior
probabilities among the $Z$ hypotheses $D_{ij}$.

We have split the sum in the odds $\Omega_i$ into a sum of individual
odds ratios $\Omega_{ij}$ because, as we will see, we need to estimate
the ratio for each $j$ individually; we can't just evaluate the total
numerator and denominator of $\Omega_i$ independently.  The reason for
this is the all-important \emph{spectral coverage}.  Each setting of
the pair $(z_i,z_j)$ limits differently the spectral range over which
the eigenspectra are both well determined.  Imagine that one of the
hypotheses $D_{ij}$ is only testable on some particular spectral range
$[\lambda_{\min},\lambda_{\max}]$.  The data in this spectral range
can be used to estimate the single-hypothesis odds ratio $\Omega_{ij}$
of hypothesis $D_{ij}$ to the null hypothesis $S_i$.  Clearly
hypotheses $D_{ij}$ that are testable with larger spectral ranges will
be better tested, but the fact that different hypotheses are subject
to tests of different strengths merely weakens---does not
invalidate---the total hypothesis test.

The SDSS spectra have near-Gaussian uncertainties.
Therefore, for each hypothesis $D_{ij}$ we can perform least-square
fitting ($\chi^2$ minimization) on the subset of $N_{ij}$ pixels in
the flux vector $\fvec_i$ that overlap the eigenspectra spectral
ranges for both redshifts $z_i$ and $z_j$.  If we perform the
least-square fit with $n$ eigenspectra at each redshift, then the odds
ratio can be approximated by a modified difference in $\chi^2$:
\begin{equation}
\ln\Omega_{ij}= \frac{1}{2}\,\left[\chi^2_i-\chi^2_{ij}-n\right]
 +\ln\frac{p(D_{ij}|I)}{p(S_i|I)} \quad,
\end{equation}
where we have taken the natural logarithm to simplify things,
$\chi^2_i$ is the minimum $\chi^2$ under hypothesis $S_i$,
$\chi^2_{ij}$ is the minimum $\chi^2$ under hypothesis $D_{ij}$, the
adjustment of $-n$ accounts for the fact that the $S_i$ fit has $n$
fewer parameters than the $D_{ij}$ fit, and the last term is the prior
ratio.  Importantly, in this odds-ratio expression, the $\chi^2$ fits
for hypotheses $S_i$ and $D_{ij}$ must have been performed \emph{over
the same $N_{ij}$ pixels in both cases}.  Even then, the expression is
something of an approximation, because it effectively assumes that the
data are affected by perfectly known, perfectly Gaussian noise and
that one of the two hypotheses is capable of providing a good fit to
the data.

\section{Discussion points}

Objectively choosing K

Train all data, subsample (practical reasons)

Compare to PCA; compare by comparing the quality of fits, not the basis functions themselves.

Search for the secondery object also at lower redshift than the primary

Other application SMBHB, redshift determination

Priors on amplitudes

Prior for non negativity (spectra, amplitudes)

Convexity

Slower than PCA
\end{document}
