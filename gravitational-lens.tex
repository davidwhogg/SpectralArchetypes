\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,mathrsfs}
\newcommand{\facility}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
 \newcommand{\apriori}{\foreign{a priori}}
\newcommand{\equationname}[1]{equation~(\ref{#1})}
\newcommand{\fluxvec}{\vec{f}}
\newcommand{\avec}{\vec{a}}
\newcommand{\like}{\mathscr{L}}
\begin{document}

\section{Data}

Each spectrum $i$ can be thought of as an ordered list or column
vector $\fluxvec_i$ of $M$ flux density (energy per area per time per
wavelength) measurements $\fluxvec_{im}$ on a grid of $M$
wavelengths $\lambda_m$:
\begin{equation}
\fluxvec_i
\equiv \left[\begin{array}{c} f_{i0} \\
                              f_{i1} \\
                              \cdots \\
                              f_{iM} \end{array}\right]
\equiv \left[\begin{array}{c} f_{\lambda,i}(\lambda_0) \\
                              f_{\lambda,i}(\lambda_1) \\
                                                \cdots \\
                              f_{\lambda,i}(\lambda_M) \end{array}\right]
\quad ,
\end{equation}
where the wavelength grid can be arbitrary but will be made uniform in
the logarithm $\ln\lambda$ for simplicity of making redshift changes.
Associated with each measurement $f_{im}$ is an uncertainty
variance $\sigma_{im}$ and we will assume in what follows that
these uncertainty variances are well measured and that the
uncertainties are essentially Gaussian.  We will assume that
off-diagonal terms (covariances) in the uncertainty variance tensor
are small, or that the uncertainty variance tensor (covariance matrix)
is approximately
\begin{equation}
C_i = \left[\begin{array}{cccc} \sigma_{i0}^2 & 0 & & 0 \\
                                0 & \sigma_{i1}^2 & & 0 \\
                                & & \cdots & \\
                                0 & 0 & & \sigma_{iM}^2 \end{array}\right]
\quad .
\end{equation}

\section{Spectral model}

Each spectrum is modeled as the linear superposition of one or more
objects at different redshifts.  We want to model the spectrum of each
object (galaxy or quasar) with a sum of $K$ linear components:
\begin{equation}
f_{\lambda}(\lambda) = \sum_{k=1}^{K} a_k\,g_k(\lambda)
\quad ,
\end{equation}
where the modeling is done implicitly in the object rest frame, the
$a_k$ are coefficients, and the $g_k(\lambda)$ are basis spectra.
Given basis spectra, the best set of coefficients for any observed
spectrum---under the assumption of known, Gaussian uncertainties---are
found by least-square fitting.  The challenge is to find the
best set of basis spectra.

Often in astronomy, this basis is found by principal components
analysis (or equivalent) and then selection of the largest-variance
components or largest-eigenvalue eigenvectors.  However, this use of
PCA naturally locates the $K$-dimensional linear basis that minimizes
the mean-squared error in the space in which all pixels of all spectra
are treated equally: They are weighted equally in the analysis, and
residuals in them are minimized by the PCA with equal aggression.
This is an inappropriate approach in the real situation in which
different data points come with very different uncertainty variances,
and it is absolutely inapplicable when there are missing data---as
there \emph{always} are in real data sets.

For these reasons, we seek to find the basis set that optimizes a
justified scalar objective, one that is consistent with the individual
spectral pixel uncertainty variances and with the fact that there are
missing data.  When uncertainties are gaussian with known variances,
the logarithm of the likelihood is proportional to chi-squared, so we
seek to find the basis functions and coefficients that minimize a
total chi-squared:
\begin{eqnarray}\displaystyle
\chi^2 & = & X - 2\,\ln\like \nonumber\\
 & = & \sum_{i=1}^N \sum_{m=1}^M
\frac{\left[f_{im}-\sum_{k=1}^K a_{ik}
                      \,g_k(\lambda_m/[1+z_i])\right]^2}
{\sigma^2_{im}}
\quad ,
\end{eqnarray}
where $X$ is some constant and we have implicitly assumed that each
spectrum $i$ under consideration at this stage is well explained by
having all its flux come from a single object at a known redshift
$z_i$.  The free parameters are the $N\,K$ coefficients $a_{ik}$ and
the $K$ functions $g_k(\lambda)$.  Roughly speaking, we seek to find
the coefficients and basis functions that globally minimize this
scalar $\chi^2$.

Precisely speaking, we make two adjustments to this goal.  The first
is that we can't demand global optimization; this problem is not
convex.  Indeed, there are enormous numbers of local minima, in both
the trivial sense that there are exact degeneracies (swap two basis
functions and their corresponding coefficients, or re-scale a basis
function and the corresponding coefficients, and so on) and in the
non-trivial sense that there are qualitatively different solutions.
All that our methods (described in detail below) guarantee is that we
have, \emph{at fixed coefficients $a_{ik}$} the globally optimal basis
functions $g_k(\lambda)$ and that we have, at fixed basis functions
$g_k(\lambda)$ the globally optimal coefficients $a_{ik}$.

The second adjustment is that we impose a smoothness prior to improve
performance at (rest-frame) wavelengths at which we have very few
data.  In practice, we implement this prior by constructing the basis
functions $g_k(\lambda)$ on a grid of $M_g>M$ rest wavelengths
$\lambda_{\ell}$ and penalizing quadratically large pixel-to-pixel
variations.  That is, we optimize not the pure $\chi^2$ above but a
modified scalar $\chi_{\epsilon}^2$
\begin{equation}
\chi_{\epsilon}^2 \equiv \chi^2
 + \epsilon\,\sum_{k=1}^K \sum_{\ell=2}^{M_g}
 \left[g_k(\lambda_{\ell})-g_k(\lambda_{\ell-1})\right]^2
\quad ,
\end{equation}
where $\epsilon$ is a scalar that sets the strength of the smoothing.
Optimization of this scalar $\chi_{\epsilon}^2$ is equivalent to
optimization of the posterior probability distribution with a Gaussian
prior applied to the pixel-to-pixel differences.  As with the observed
spectra, we pixelize the basis functions $g_k(\lambda)$ on a grid that
is uniform in logarithmic wavelength $\ln\lambda$ (with the same grid
spacing as that used for the observed spectra).

Given a set of basis spectra $g_k(\lambda)$, the optimal coefficients
$a_{ik}$ are given by standard least-square fitting:
\begin{equation}
\avec_i \leftarrow \inverse{\left[\transpose{A_i} \inverse{C_i} A_i\right]}
 \left[\transpose{A_i} \inverse{C_i} \fluxvec_i\right]
\quad ,
\end{equation}
where the vectors and matrices are defined by
\begin{eqnarray}
\avec & \equiv & \nonumber\\
A_i & \equiv & \nonumber\\
C_i & \equiv &
\end{enarray}

[Hogg: Now give pseudo-code explaining the iterative bilinear solution.]

\section{Training and test data}

[Vivi: How are the training data selected?]

[Vivi: How do we know we have converged?]

[Vivi: How are the test data selected?]

[Vivi: What is chi-squared on the test data as a function of $K$ and
  $\epsilon$?]

\section{Hypothesis test}

For each spectrum $i$, we generate $Z+1$ mutually exclusive hypotheses:
The null hypothesis $S_i$ that spectrum $i$ only has significant flux
coming from a single redshift $z_i$ determined by the \facility{SDSS}
pipeline, and $Z$ hypotheses $D_{ij}$ that spectrum $i$ has significant
flux coming from two redshifts, $z_i$ and another redshift
$z_j>z_i+\epsilon$.  In the context of this very restricted universe
of hypotheses, the odds ratio $\Omega_i$ for the null hypothesis is
\begin{equation}\label{eq:odds}
\Omega_i = \frac{\sum_{j=1}^Z p(D_{ij}|\fluxvec_i,I)}{p(S_i|\fluxvec_i,I)}
 = \sum_{j=1}^Z \left[\frac{p(\fluxvec_i|D_{ij},I)}{p(\fluxvec_i|S_i,I)}
 \,\frac{p(D_{ij}|I)}{p(S_i|I)}\right] = \sum_{j=1}^Z\Omega_{ij}\quad,
\end{equation}
where the spectral flux data are represented by the vector
$\fluxvec_i$, the symbol $I$ represents all of the prior information
in the problem, including but not limited to the hypothesis
specification, the wavelengths and uncertainties associated with the
spectral flux data, and any other knowledge that the investigator
might have about the hypotheses prior to any data analysis.  We have
implicitly defined an individual-hypothesis odds ratio
\begin{equation}
\Omega_{ij} = \frac{p(\fluxvec_i|D_{ij},I)}{p(\fluxvec_i|S_i,I)}
  \,\frac{p(D_{ij}|I)}{p(S_i|I)}\quad.
\end{equation}
Because an individual spectrum is unlikely \apriori\ to show two
redshifts, the prior probabilities will have the asymmetry
\begin{equation}
\sum_{j=1}^Z p(D_{ij}|I) \ll p(S_i|I) \quad,
\end{equation}
and it remains for us to decide how to set the relative prior
probabilities among the $Z$ hypotheses $D_{ij}$.

We have split the sum in the odds $\Omega_i$ into a sum of individual
odds ratios $\Omega_{ij}$ because, as we will see, we need to estimate
the ratio for each $j$ individually; we can't just evaluate the total
numerator and denominator of $\Omega_i$ independently.  The reason for
this is the all-important \emph{spectral coverage}.  Each setting of
the pair $(z_i,z_j)$ limits differently the spectral range over which
the eigenspectra are both well determined.  Imagine that one of the
hypotheses $D_{ij}$ is only testable on some particular spectral range
$[\lambda_{\min},\lambda_{\max}]$.  The data in this spectral range
can be used to estimate the single-hypothesis odds ratio $\Omega_{ij}$
of hypothesis $D_{ij}$ to the null hypothesis $S_i$.  Clearly
hypotheses $D_{ij}$ that are testable with larger spectral ranges will
be better tested, but the fact that different hypotheses are subject
to tests of different strengths merely weakens---does not
invalidate---the total hypothesis test.

The SDSS spectra have near-Gaussian uncertainties.
Therefore, for each hypothesis $D_{ij}$ we can perform least-square
fitting ($\chi^2$ minimization) on the subset of $N_{ij}$ pixels in
the flux vector $\fluxvec_i$ that overlap the eigenspectra spectral
ranges for both redshifts $z_i$ and $z_j$.  If we perform the
least-square fit with $n$ eigenspectra at each redshift, then the odds
ratio can be approximated by a modified difference in $\chi^2$:
\begin{equation}
\ln\Omega_{ij}= \frac{1}{2}\,\left[\chi^2_i-\chi^2_{ij}-n\right]
 +\ln\frac{p(D_{ij}|I)}{p(S_i|I)} \quad,
\end{equation}
where we have taken the natural logarithm to simplify things,
$\chi^2_i$ is the minimum $\chi^2$ under hypothesis $S_i$,
$\chi^2_{ij}$ is the minimum $\chi^2$ under hypothesis $D_{ij}$, the
adjustment of $-n$ accounts for the fact that the $S_i$ fit has $n$
fewer parameters than the $D_{ij}$ fit, and the last term is the prior
ratio.  Importantly, in this odds-ratio expression, the $\chi^2$ fits
for hypotheses $S_i$ and $D_{ij}$ must have been performed \emph{over
the same $N_{ij}$ pixels in both cases}.  Even then, the expression is
something of an approximation, because it effectively assumes that the
data are affected by perfectly known, perfectly Gaussian noise and
that one of the two hypotheses is capable of providing a good fit to
the data.

\end{document}
