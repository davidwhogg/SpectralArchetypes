% to-do items:
% ------------
% - write text
% - deal with all items marked "TBD"

\documentclass[preprint]{aastex}
\usepackage{amssymb,amsmath,mathrsfs}
\newcounter{address}
\newcommand{\foreign}[1]{\textit{#1}}
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\km}{\unit{km}}
\newcommand{\s}{\unit{s}}
\newcommand{\kmps}{\km\,\s^{-1}}
\newcommand{\mum}{\unit{\mu m}}
\newcommand{\forbidden}[1]{{[}{\mathrm{#1}}{]}}
\newcommand{\OIII}{\forbidden{OIII}}
\newcommand{\mmatrix}[1]{\boldsymbol{#1}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\covar}{\mmatrix{C}}
\newcommand{\avec}{\mmatrix{a}}
\newcommand{\evec}{\mmatrix{e}}
\newcommand{\fvec}{\mmatrix{f}}
\newcommand{\Fvec}{\mmatrix{F}}
\newcommand{\gvec}{\mmatrix{g}}
\newcommand{\invvar}{\inverse{\covar}}
\newcommand{\all}[1]{\{{#1}\}}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\obs}{\mathrm{obs}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\like}{\mathscr{L}}

\begin{document}
\title{Quasar redshift determination with a data-driven model}
\author{some permutation of PT\altaffilmark{\ref{MPIA}},
        MVM\altaffilmark{\ref{MPIA},\ref{Caltech}},
        JH\altaffilmark{\ref{MPIA},\ref{email}},
        DWH\altaffilmark{\ref{MPIA},\ref{CCPP}}}
\setcounter{address}{1}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIA}
Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17,
D-69117 Heidelberg, Germany}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Caltech}
Caltech}
\altaffiltext{\theaddress}{\stepcounter{address}\label{email} To whom
correspondence should be addressed: \texttt{joe@mpia.de}}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CCPP} Center
for Cosmology and Particle Physics, Department of Physics, New York
University, 4 Washington Place, New York, NY 10003}

\begin{abstract}
We present a method for measuring the redshifts of quasars by fitting
them with a data-driven model.  This is a a model of the space of
possible quasar spectra derived from a collection of quasars observed
by the Sloan Digital Sky Survey.  We show that the measurements
obtained with the model fits are better than those obtained by
cross-correlation with a fixed quasar template, and that they perform
well even when the narrow $\OIII$ lines are outside the spectroscopic
window.  TBD: results here.
\end{abstract}

\section{Introduction}

TBD: What is the problem and why is it important?

The standard data-driven models for quasars---or any other kind of
spectroscopic object in astronomy---is the highest-ranked principal
components from a principal components analysis (PCA) or equivalent.
The PCA has one advantage and a number of drawbacks.  The advantage is
that it is entirely data-driven: The construction of the PCA requires
no theoretical or external knowledge about the spectra being modeled;
it is a dimensionality reduction in the space of the observed spectra.

There are many drawbacks to PCA but the most important is that the PCA
returns the principal directions---the eigenvectors with maximum
eigenvalues---of the variance tensor of the data; this variance tensor
has contributions from intrinsic variation among spectra, and
contributions from observational noise.  That is, a direction in
spectrum space can enter into the top principal components because it
is a direction of great astrophysical variation, or because there is a
lot of noise in the observations along that direction, or both.  PCA
is agnostic about the \emph{source} of the variance, while astronomers
are not; astronomers want to know about the astrophysical processes
that generate the data \emph{prior} to the addition of observational
noise.

Other drawbacks to PCA include the following: It treats the data as
drawn from a linear subspace of the full spectral space.  This
assumption is unlikely to be true in any application.  It also has
trouble separating the spectral variation that comes from amplitude
changes (overall flux or luminosity changes) as distinct from
variations that come from shape changes in the spectra.  Various hacks
have been employed to deal with this, but many of them make the linear
subspace assumption even less valid than it was \foreign{a priori}.
Finally, PCA has no idea about prior information; it is just as happy
creating components with negative amplitudes as positive amplitudes
and the linear subspace therefore contains many quadrants, in general,
that represent spectra with completely unphysical properties (such as
negative emission lines and the like).

In this \documentname, we use a data-driven model for quasar spectra
that overcomes most---though not all---of the problems with PCA.

\section{Method}

We ``learn''---or ``find optimal''---spectral components or basis
spectra by a bilinear optimization on a training set of spectra.  We
use linear combinations of these components to determine the redshifts
of spectra in a disjoint test set.  We apply a strong prior on the
allowed coefficients determined from the population in the training
set.

\subsection{Training}

The training data set will be a set of $N$ spectra $i$, each of which
is a vector $\fvec_i$ of $M$ fluxes $f_{ij}$ at $M$ wavelengths
$\lambda_j$.  Associated with each flux $f_{ij}$ is an inverse
uncertainty variance $1/\sigma^2_{ij}$, which we will imagine are the
elements of a $M\times M$ diagonal inverse covariance matrix
$\invvar_{i}$.

The model is that any spectrum $\fvec_i$ can be written as a linear
sum of $K$ components $\gvec_k$ (each of which also has $M$ components
$g_{kj}$)
\begin{eqnarray}\displaystyle
\fvec_i &=& \sum_{k=1}^K a_{ik}\,\gvec_k + \evec_i \nonumber\\
f_{ij} &=& \sum_{k=1}^K a_{ik}\,g_{kj} + e_{ij}
\quad ,
\end{eqnarray}
where the $a_{ik}$ are coefficients, and the vector $\evec_i$ (with
$M$ components $e_{ij}$) is a noise vector, imagined to be a random
variable drawn from a $M$-dimensional Gaussian with zero mean and
variance tensor $\covar_i$.

We determine the components $\gvec_k$ by bi-linear least-square
fitting to a \emph{training set}.  The goal is to find the set of
coefficients $a_{ik}$ and spectral components $g_{kj}$ that jointly
minimize a total goodness-of-fit scalar $\chi^2$ over all the $N$
training data:
\begin{eqnarray}\displaystyle
\chi^2 &=& \sum_{i=1}^N 
 \,\transpose{\left[\fvec_i - \sum_{k=1}^K a_{ik}\,\gvec_k\right]}
 \cdot\invvar_i\cdot\left[\fvec_i - \sum_{k=1}^K a_{ik}\,\gvec_k\right]
 \nonumber\\
\chi^2 &=& \sum_{i=1}^N \sum_{j=1}^M \frac{1}{\sigma^2_{ij}}
 \,\left[f_{ij} - \sum_{k=1}^K a_{ik}\,g_{kj}\right]^2
\quad .
\end{eqnarray}
This problem is not convex, and there are multiple minima, including
degenerate solutions.  We optimize to a local minimum using iterated
least squares.  In each step, we fix the $g_{kj}$ and find the optimal
$a_{ik}$ by weighted least squares, and then hold the $a_{ik}$ fixed
and find the optimal $g_{kj}$ by weighted least squares.  Each
iteration step is guaranteed to reduce the total $\chi^2$ and
converges in practice in ten or so iterations.  We initialize the
fitting with the output of a PCA on the spectra after the each
spectrum has been projected into the subspace orthogonal to the mean
spectrum of the data set; the first guesses for the $\gvec_k$ are set
to the mean spectrum and the first $K-1$ principal components.  We
find that performance is not worse if we initialize with a randomly
chosen set of spectra.

Importantly in the training, the wavelengths $\lambda_j$ are
rest-frame wavelengths, and because different training-set spectra
were taken at different redshifts, not all spectra have coverage at
all rest-frame wavelengths.  (There are also gaps from bad sky
subtraction, cosmic rays, and other data issues.)  Missing data in
spectrum $i$ at wavelength $j$ is handled naturally by setting
$1/\sigma^2_{ij}=0$; when a data point has vanishing inverse
variance, it does not contribute to $\chi^2$ or the fitting.  In
general, weighted least squares---because it uses the error variances
correctly---is unperturbed by missing data.

For any $K$-dimensional linear subspace, there are many choices for
the components $\gvec_k$: The components can be reordered, multiplied
by scalars, or replaced with linear combinations of themselves.  We
don't try to break all of these degeneracies, but we do enforce the
constraint that the $\gvec_k\cdot\gvec_k=1$, where the inner product
is taken with the trivial (identity) metric in the spectrum space.

\subsection{Test time}

At test time, we fit a previously unmodeled spectrum $\ell$ from the
\emph{test set}, each of which is a vector $\fvec_\ell$ of $M_\obs$
rest-frame fluxes $f_{\ell m}$ on a grid of $M_\obs$ observed-frame
wavelengths $\lambda_m$.  Associated with each flux $f_{\ell m}$ is an
inverse uncertainty variances $1/\sigma^2_{\ell m}$.  Again, where
there are missing data, these inverse variances are set to zero.

In order to simplify redshift correction and fitting, we choose
rest-frame wavelength grid $\lambda_j$ and observed-frame wavelength
grid $\lambda_m$ to be logarithmic with common spacing:
\begin{eqnarray}\displaystyle
\ln\lambda_j &=& A + j\,B \nonumber\\
\ln\lambda_m &=& C + m\,B
\quad ,
\end{eqnarray}
where the logarithmic wavelength interval $B$ is common to both, and
set small enough that the spectroscopic wavelength element is well
sampled.  This permits trivial redshift analysis on a fine grid, with
redshift spacing $\Delta\ln(1+z)=\Delta z/[1+z]=B$.  The permitted
redshifts $z_n$ are therefore
\begin{eqnarray}\displaystyle
\ln(1+z_n) &=& [C - A] + n\,B
\quad ,
\end{eqnarray}
where $n$ can have a wide range of positive and negative values in
principle.  Specifically, for the experiments to follow, we set the
training-set rest-frame wavelength grid to have $M=5591$,
$\exp(A)=178.6~\mum$, and $B=2.3026\times10^{-4}$, and the test-set
observed-frame wavlength grids have $M_\obs\sim$TBD and $C\sim$TBD.
On these grids, the velocity spacing is $69.03~\kmps$.

For any test spectrum $\fvec_\ell$, at any setting of redshift $z_n$
and coefficients $a_{\ell k}$ there is a goodness-of-fit scalar
$\chi^2_\ell$
\begin{eqnarray}\displaystyle
\chi^2_\ell &=& \sum_{m=1}^{M_\obs} \frac{1}{\sigma^2_{\ell m}}
 \,\left[f_{\ell m} - \sum_{k=1}^K a_{\ell k}\,g_{k[m-n]}\right]^2
\quad,
\end{eqnarray}
where $n$ is the redshift index, and implicitly the $1/\sigma^2_{\ell
  m}$ are set to vanish at any wavelengths for which the model and
data don't overlap.  The goal is probabilistic inference, which
requires computed likelihoods.  Under the assumptions laid out in this
section, the scalar $\chi^2_\ell$ is related to the likelihood
$\like_\ell$ by
\begin{eqnarray}\displaystyle
\chi^2_\ell &=& Q - 2\,\ln\like_\ell \nonumber\\
\like_\ell &\equiv& p(\fvec_\ell|z_n,\avec_\ell)
\quad ,
\end{eqnarray}
where $Q$ is some constant, and the vector $\avec_\ell$ contains the
$K$ coefficients $a_{\ell k}$ for test spectrum $\ell$.

\subsection{Informative prior}

In the end we want probabilistic information about the redshift $z_n$,
not the coefficients $\avec_\ell$.  If we have a prior PDF on the
coefficients $\avec_\ell$, we can marginalize out the coefficients to
get a likelihood for the redshift alone
\begin{eqnarray}\displaystyle
p(\fvec_\ell|z_n) &=& \int \dd\avec_\ell\,p(\avec_\ell)\,p(\fvec_\ell|z_n,\avec_\ell)
\quad ,
\end{eqnarray}
where the integral is over the $K$-dimensional space of coefficients
$a_{\ell k}$ and the integration measure is the prior PDF $p(\avec)$.

Because we construct a training set that is statistically identical to
the test set (this is not always or not usually possible), we can take
a hierarchical approach in which the prior is set by the instances in
the training set; indeed the coefficient vectors $\avec_i$ for the
training set spectra $i$ can be thought of as making up a
mixture-of-delta-functions approximation to the prior PDF.  The one
modification we make to this is that we permit overall amplitude (flux
or luminosity variations), so in $K$ dimensions, each training-set
instance $\avec_i$ defines a line $r\,\avec_i$ in coefficient space,
where $r$ is a scalar.  We set the prior on $r$ to be constant in $r$;
without bounds this prior is improper, but in practice the posterior
PDFs are always normalizable.  The prior PDF is thus a
training-set-based mixture-of-lines in coefficient space.

With this informative prior, the marginalization integral can be
approximated with a sum
\begin{eqnarray}\displaystyle
p(\fvec_\ell|z_n) &\approx& \frac{1}{N}\,\sum_{i=1}^N p(\fvec_\ell|z_n,r_{\ell ni}\,\avec_i)\,\Delta r_{\ell ni} \nonumber\\
p(\fvec_\ell|z_n,r_{\ell ni}\,\avec_i) &\propto& \exp(-\frac{1}{2}\,\chi^2_{\ell ni}) \nonumber\\
\chi^2_{\ell ni} &\equiv& \transpose{[\fvec_\ell - r_{\ell ni}\,\Fvec_{ni}]}\cdot\invvar_\ell\cdot[\fvec_\ell - r_{\ell ni}\,\Fvec_{ni}] \nonumber\\
r_{\ell ni} &=& \frac{\transpose{\fvec_\ell}\cdot\invvar_\ell\cdot\Fvec_{ni}}{\transpose{\Fvec_{ni}}\cdot\invvar_\ell\cdot\Fvec_{ni}} \nonumber\\
F_{nim} &\equiv& \sum_{k=1}^K a_{ik}\,g_{k[m-n]} \nonumber\\
{}[\Delta r_{\ell ni}]^2 &\equiv& \frac{1}{\transpose{\Fvec_{ni}}\cdot\invvar_\ell\cdot\Fvec_{ni}}
\quad ,
\end{eqnarray}
where $r_{\ell ni}$ is the scalar that minimizes the scalar
$\chi^2_{\ell ni}$, $\Fvec_{ni}$ is a short-hand for the best fit to
training spectrum $\fvec_i$ shifted by $n$ spacings in the velocity
grid (it has components $F_{nim}$), and $\Delta r_{\ell ni}$ is the
output uncertainty on $r_{\ell ni}$ according to one-dimensional
weighted least squares.  This sum becomes exact in the limit that the
observational uncertainties are Gaussian, the inverse variance
matrices $\invvar_\ell$ are correct, and the number $N$ of spectra in
the training set becomes large.  In practice, when the $\chi^2_{\ell n
  i}$ are exponentiated, it makes sense to subtract from them a
constant $Q_\ell$ to protect the exponential function from underflow.

\section{Training and test data}

TBD:  Michael, explain data selection and redshift determination, etc.

\section{Results}

TBD:  Vivi make figures.

\section{Discussion}

The method used here has the advantage of PCA that it is entirely
data-driven, but far fewer of the disadvantages of PCA.  It uses a
linear subspace of spectrum space, like PCA, but unlike PCA it applies
a prior to that subspace so that unrealistic spectra are never
instantiated.  Most importantly, unlike PCA, the method makes proper
use of the observational errors, so the basis spectra are models of
the underlying physical process that generates the quasars.

TBD:  Joe, Hogg go crazy.

\acknowledgements
Thanks to people.  Thanks to SDSS.  Thanks to R.  Thanks to grants.

\end{document}
